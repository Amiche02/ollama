"""
Entry point for the Python Ollama Brainstorming Chat CLI.
"""

import sys

from config.config import SYSTEM_PROMPT
from services.ollama import chat_with_model, fetch_models

try:
    import pyfiglet
except ImportError:
    pyfiglet = None


def display_ascii_logo():
    """Display an ASCII logo similar to the Go version's figure.NewColorFigure."""
    if pyfiglet:
        ascii_banner = pyfiglet.figlet_format("HELLO PULSE")
        print(ascii_banner)
    else:
        # Fallback if pyfiglet is not installed
        print("HELLO PULSE")
    print("üöÄ Welcome to HELLO PULSE AI Chat")
    print("üß† Powered by Python and Ollama API")
    print("-------------------------------------------------\n")


def main():
    display_ascii_logo()

    # Fetch the models
    try:
        models = fetch_models()
    except ConnectionError as e:
        print(f"‚ùå {e}")
        sys.exit(1)

    if not models:
        print(
            "‚ùå Aucun mod√®le trouv√©. V√©rifiez qu'Ollama est bien en cours d'ex√©cution."
        )
        sys.exit(1)

    print("üìå Mod√®les disponibles :")
    for i, model in enumerate(models):
        print(f"{i+1}: {model}")

    # Let user choose a model
    choice = input("\nS√©lectionnez un mod√®le : ")
    try:
        choice_idx = int(choice) - 1
        selected_model = models[choice_idx]
    except (ValueError, IndexError):
        print("‚ùå Choix invalide.")
        sys.exit(1)

    print(f"\n‚úÖ Mod√®le s√©lectionn√© : {selected_model}")
    print("\nüí¨ Brainstorming Chatbot (Tapez 'exit' pour quitter)")

    # Initialize conversation history
    conversation_history = f"System: {SYSTEM_PROMPT}"

    while True:
        user_input = input("\nüë§ Vous : ").strip()

        if user_input.lower() == "exit":
            print("\nüëã √Ä bient√¥t !")
            break

        # Add user message to conversation
        conversation_history += f"\nUser: {user_input}"

        # Stream AI response
        print("\nüß† Animateur IA : ", end="", flush=True)

        ai_response_accumulator = []
        for chunk in chat_with_model(selected_model, conversation_history):
            print(chunk, end="", flush=True)
            ai_response_accumulator.append(chunk)

        ai_response_text = "".join(ai_response_accumulator)
        print()  # New line after the streamed response

        # Add AI's full response to conversation history
        conversation_history += f"\nAI: {ai_response_text}"


if __name__ == "__main__":
    main()
